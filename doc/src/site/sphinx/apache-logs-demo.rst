NASA Apache Logs Demo
*********************

To play around with the different components that we have created within Stratio Ingestion we have designed an scenario where we launch one flume agent against an Ubuntu Virtual Machine with the following structure.
We will have a data source (apache logs from NASA in 1995) which will be read and send to three sinks (Cassandra Database, ElasticSearch and Stratio Decision) through three channels. We will need this components:

* Cassandra database
* Stratio Decision Engine
* Zookeeper (for Stratio Decision)
* Apache kafka (for Stratio Decision)
* ElasticSearch
* Kibana (for data visualization)


Agents configuration
====================

By running the agent, flume will walk through the apache log file and for each log entry it will use a morphline interceptor (see interceptor.conf file) to read the lines, append an autogenerated UUID, and parse the log fields using grok.

The full agent configuration is the following (see /opt/sds/ingestion/examples/apache-logs/flume-conf.properties):

* Source:

  - SpoolDir (we have used a 300 Mb apache log file)

* Channels:

  - 3 file channels for cassandra, stratio decision & elastic search

* Sinks:

  - Cassandra sink (`developed by Stratio`_), see also the definition_access_log.json attached)
  - Stratio Decision sink (`also developed by Stratio`_)
  - ElasticSearch sink (included in flume core)

.. _developed by Stratio: https://github.com/Stratio/flume-ingestion/tree/master/stratio-sinks/stratio-cassandra-sink
.. _also developed by Stratio:  https://github.com/Stratio/flume-ingestion/tree/master/stratio-sinks/stratio-decision-sink



Preparing the environment
=========================

Manually
--------

You need an extracted full distribution of Stratio Ingestion at ``/opt/sds/ingestion``. You can use a different path via the
``INGESTION_HOME`` environment variable. You will also need a running Stratio Decision if you use the sink (you can use our sandbox for that), ElasticSearch and Cassandra. By default, it will use only
Cassandra and ElasticSearch. See below for different set ups you can use.


Vagrant Box
-----------

You can use our Vagrant Box to run the example, just type: ``vagrant init stratio/ingestion`` to get our sandbox.

Then you must type ``vagrant up`` to start the machine.

You can edit the /opt/sds/ingestion/examples/apache-logs/conf/flume-conf.properties for customizing the example. By default, we have activated two sinks: ElasticSearch and Cassandra, but we provide the configuration for Stratio Decision Sink commented in the same file.

Due to an issue in virtualbox in Windows 10 computers, sandbox could not install properly the network interfaces. There is an official Virtualbox patch it. Just download and run as administrator:
https://www.virtualbox.org/attachment/ticket/14040/VBox-Win10-fix-14040.exe
If you've this problem in your local, please run this VirtualBox fix in the background while executing vagrant up.

Running the example
===================

To run the agent just type:
::

   cd /opt/sds/ingestion/examples/apache-logs/bin
   sudo ./run_example.sh

This command downloads an apache log and sends it to Stratio Ingestion, you can check the sinks, channels and sources configurated in http://IP:34545/metrics (you can obtain the IP of your machine when you start the vagrant box). This metrics shows you the flow of the info throw the flume agent (source -> channel -> sink)

Since we have activated two sink (ElasticSearch and Cassandra) you can check the info loaded in them:

- Cassandra: Enter in cassandra console
::

    sudo ./opt/sds/cassandra/bin/cqlsh

 then type

::

    select * from test.access_logs;

- ElasticSearch: Start Kibana if it is stopped (``cd /opt/sds/kibana-4.0.2-linux-x64/bin`` and ``./kibana``) Enter in Kibana http://IP:5601 in Dashboard Tab You can Load saved dashboard and select "Apache Logs Demo Dashboard". Due to date of logs, you must select a new time filter between July-1995 and August-1995. You can select that date range by clicking on the top right corner, selecting  the "Absolute" option on the left menu and establishing the dates in the calendar:

.. image:: /images/apache_logs_time.jpg
 :align: center


You should see something like this:

.. image:: /images/flume-logs.jpg
 :align: center


Connection with Stratio Decision
================================

If you want to check the connection with Stratio Decision uncomment the configuration in (it is recommended to work as root: just type ``sudo su-``) /opt/sds/ingestion/examples/apache-logs/conf/flume-conf.properties:


::

    a.sinks.decisionSink.type=com.stratio.ingestion.sink.decision.StratioDecisionSink
    a.sinks.decisionSink.kafkaHost=localhost
    a.sinks.decisionSink.kafkaPort=9092
    a.sinks.decisionSink.zookeeperHost=localhost
    a.sinks.decisionSink.zookeeperPort=2181
    a.sinks.decisionSink.streamDefinitionFile=/opt/flume/conf/stream.conf
    a.sinks.decisionSink.streamFields=log_id,log_host,log_user,log_date,log_http_method,log_url_path,log_http_version,log_http_code,log_bytes_returned

We recommend to use decision vagrant box (``vagrant init stratio/decision`` and ``vagrant up``) and use this parameters to configure flume (IPs and ports for kafka and zookeeper must be replaced by IPs and ports configured in the decision machine). If you have decision vagrant box working, then you can run the flume example.
The example will create the stream necessary for working in decision, you can run the decision shell and type create new querys:

Create query **host_requests_per_second**:

::

    add query --stream testStream --definition "from testStream #window.time(1 second)
    select count(log_id) as host_requests_per_second,log_id, log_host,log_user,log_date,
    log_http_method, log_url_path, log_http_version, log_http_code, log_bytes_returned
    group by log_host insert into host_requests_per_second"

Create query **resource_requests_per_second**:

::

    add query --stream testStream --definition "from testStream #window.time(1 second)
    select count(log_id) as resource_requests_per_second,log_id, log_host,log_user,
    log_date,log_http_method, log_url_path, log_http_version, log_http_code,
    log_bytes_returned group by log_url_path insert into resource_requests_per_second"

Create query **host_request_per_seconde_per_resource**:

::

    add query --stream testStream --definition "from testStream #window.time(1 second)
    select count(log_id) as host_request_per_second_per_resource,log_id,
    log_host,log_user,log_date,log_http_method, log_url_path, log_http_version,
    log_http_code, log_bytes_returned group by log_host,log_url_path insert
    into host_request_per_second_per_resource"

Index all streams (so you can check them in ElasticSearch):

::

    index start --stream testStream

::

    index start --stream host_requests_per_second

::

    index start --stream resource_requests_per_second

::

    index start --stream host_request_per_second_per_resource
