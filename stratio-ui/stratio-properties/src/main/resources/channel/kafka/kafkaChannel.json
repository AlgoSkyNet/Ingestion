{
  "descriptors": {
    "components": [
      {
        "id": "kafka",
        "component": "channel",
        "type": "org.apache.flume.channel.kafka.KafkaChannel",
        "name": "Kafka Channel",
        "description": "The events are stored in a Kafka cluster (must be installed separately). Kafka provides high availability and replication, so in case an agent or a kafka broker crashes, the events are immediately available to other sinksThe Kafka channel can be used for multiple scenarios: * With Flume source and sink - it provides a reliable and highly available channel for events * With Flume source and interceptor but no sink - it allows writing Flume events into a Kafka topic, for use by other apps * With Flume sink, but no source - it is a low-latency, fault tolerant way to send events from Kafka to Flume sources such as HDFS, HBase or Solr",
        "settings": {
          "brokerList": {
            "type": "",
            "name": "Broker list",
            "description": "List of brokers in the Kafka cluster used by the channel This can be a partial list of brokers, but we recommend at least two for HA. The format is comma separated list of hostname:port",
            "default": "",
            "required": true
          },
          "zookeeperConnect": {
            "type": "",
            "name": "Zookeeper connect",
            "description": "URI of ZooKeeper used by Kafka cluster The format is comma separated list of hostname:port. If chroot is used, it is added once at the end. For example: zookeeper-1:2181,zookeeper-2:2182,zookeeper-3:2181/kafka",
            "default": "",
            "required": true
          },
          "topic": {
            "type": "",
            "name": "Topic",
            "description": "Kafka topic which the channel will use",
            "default": "flume-channel",
            "required": false
          },
          "groupId": {
            "type": "",
            "name": "Group ID",
            "description": "Consumer group ID the channel uses to register with Kafka. Multiple channels must use the same topic and group to ensure that when one agent fails another can get the data Note that having non-channel consumers with the same ID can lead to data loss.",
            "default": "flume",
            "required": false
          },
          "parseAsFlumeEvent": {
            "type": "",
            "name": "Parse as Flume event",
            "description": "Expecting Avro datums with FlumeEvent schema in the channel. This should be true if Flume source is writing to the channel And false if other producers are writing into the topic that the channel is using Flume source messages to Kafka can be parsed outside of Flume by using org.apache.flume.source.avro.AvroFlumeEvent provided by the flume-ng-sdk artifact",
            "default": "true",
            "required": false
          },
          "readSmallestOffset": {
            "type": "",
            "name": "Read smallest offset",
            "description": "When set to true, the channel will read all data in the topic, starting from the oldest event when false, it will read only events written after the channel started When “parseAsFlumeEvent” is true, this will be false. Flume source will start prior to the sinks and this guarantees that events sent by source before sinks start will not be lost.",
            "default": "false",
            "required": false
          },
          "Other Kafka Properties": {
            "type": "",
            "name": "Other Kafka properties",
            "description": "These properties are used to configure the Kafka Producer and Consumer used by the channel. Any property supported by Kafka can be used. The only requirement is to prepend the property name with the prefix kafka.. For example: kafka.producer.type",
            "default": "",
            "required": false
          }
        },
        "ports": {

        },
        "ui": {

        }
      }
    ]
  }
}